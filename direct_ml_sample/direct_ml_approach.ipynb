{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An alternative method to leverage the integrated GPU on Windows laptops for deep learning tasks using direct_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "DirectML (Direct Machine Learning) is a hardware-accelerated machine learning API from Microsoft, built on top of DirectX 12. It enables efficient ML inferencing on GPUs and other accelerators across Windows devices.\n",
    "\n",
    "By leveraging DirectML, you can utilize integrated GPUs on standard laptops for deep learning tasks, which is beneficial for setting up baseline models. However, this approach has limitations and may not fully replace dedicated GPUs or cloud-based solutions. For long-term use cases, transitioning to these more robust methods is recommended.\n",
    "\n",
    "DirectML provides support for popular frameworks like TensorFlow and PyTorch. For PyTorch, the (torch-directml) package enables GPU acceleration via DirectML. Similarly, (TensorFlow-DirectML) allows TensorFlow to perform high-performance training and inferencing on any Windows device with a DirectX 12-capable GPU. \n",
    "\n",
    "Additionally, community projects have explored running YOLOv8 on DirectML, though these implementations may still face some constraints and require further modifications. \n",
    "\n",
    "## Limitation\n",
    "TensorFlow and PyTorch offer some level of support for DirectML, but not full compatibility, meaning certain data types may still be unsupported.\n",
    "\n",
    "Please find at link:   \n",
    "[Pytorch](https://github.com/microsoft/DirectML/wiki/PyTorch-DirectML-Operator-Roadmap)   \n",
    "[Tensorflow](https://github.com/microsoft/tensorflow-directml-plugin)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm aware that this topic has been widely discussed, and this summary is intended to consolidate key points and help the community save time and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch \n",
    "With Pytorch, please install torch_directml. When running, you will initiate the device and add your model into it as below template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch_directml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:11<00:00, 853121.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 95845.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 936383.22it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 1845981.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DirectML device\n",
    "dml = torch_directml.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNN()\n",
    "model = model.to(dml)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1/938], Loss: 2.3056\n",
      "Epoch [1/5], Step [101/938], Loss: 0.6486\n",
      "Epoch [1/5], Step [201/938], Loss: 0.5244\n",
      "Epoch [1/5], Step [301/938], Loss: 0.3761\n",
      "Epoch [1/5], Step [401/938], Loss: 0.3318\n",
      "Epoch [1/5], Step [501/938], Loss: 0.4377\n",
      "Epoch [1/5], Step [601/938], Loss: 0.2180\n",
      "Epoch [1/5], Step [701/938], Loss: 0.2488\n",
      "Epoch [1/5], Step [801/938], Loss: 0.2940\n",
      "Epoch [1/5], Step [901/938], Loss: 0.5269\n",
      "Epoch [2/5], Step [1/938], Loss: 0.1859\n",
      "Epoch [2/5], Step [101/938], Loss: 0.2807\n",
      "Epoch [2/5], Step [201/938], Loss: 0.2691\n",
      "Epoch [2/5], Step [301/938], Loss: 0.2416\n",
      "Epoch [2/5], Step [401/938], Loss: 0.2049\n",
      "Epoch [2/5], Step [501/938], Loss: 0.0756\n",
      "Epoch [2/5], Step [601/938], Loss: 0.3464\n",
      "Epoch [2/5], Step [701/938], Loss: 0.0964\n",
      "Epoch [2/5], Step [801/938], Loss: 0.1446\n",
      "Epoch [2/5], Step [901/938], Loss: 0.3289\n",
      "Epoch [3/5], Step [1/938], Loss: 0.2025\n",
      "Epoch [3/5], Step [101/938], Loss: 0.0832\n",
      "Epoch [3/5], Step [201/938], Loss: 0.1234\n",
      "Epoch [3/5], Step [301/938], Loss: 0.1870\n",
      "Epoch [3/5], Step [401/938], Loss: 0.1855\n",
      "Epoch [3/5], Step [501/938], Loss: 0.1375\n",
      "Epoch [3/5], Step [601/938], Loss: 0.1812\n",
      "Epoch [3/5], Step [701/938], Loss: 0.0909\n",
      "Epoch [3/5], Step [801/938], Loss: 0.0490\n",
      "Epoch [3/5], Step [901/938], Loss: 0.1887\n",
      "Epoch [4/5], Step [1/938], Loss: 0.1480\n",
      "Epoch [4/5], Step [101/938], Loss: 0.0688\n",
      "Epoch [4/5], Step [201/938], Loss: 0.1322\n",
      "Epoch [4/5], Step [301/938], Loss: 0.0804\n",
      "Epoch [4/5], Step [401/938], Loss: 0.1536\n",
      "Epoch [4/5], Step [501/938], Loss: 0.1818\n",
      "Epoch [4/5], Step [601/938], Loss: 0.2385\n",
      "Epoch [4/5], Step [701/938], Loss: 0.1024\n",
      "Epoch [4/5], Step [801/938], Loss: 0.1000\n",
      "Epoch [4/5], Step [901/938], Loss: 0.0574\n",
      "Epoch [5/5], Step [1/938], Loss: 0.1374\n",
      "Epoch [5/5], Step [101/938], Loss: 0.2668\n",
      "Epoch [5/5], Step [201/938], Loss: 0.1212\n",
      "Epoch [5/5], Step [301/938], Loss: 0.1204\n",
      "Epoch [5/5], Step [401/938], Loss: 0.0997\n",
      "Epoch [5/5], Step [501/938], Loss: 0.1574\n",
      "Epoch [5/5], Step [601/938], Loss: 0.0320\n",
      "Epoch [5/5], Step [701/938], Loss: 0.0816\n",
      "Epoch [5/5], Step [801/938], Loss: 0.1702\n",
      "Epoch [5/5], Step [901/938], Loss: 0.0424\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Move data and target to the DirectML device\n",
    "        data, target = data.to(dml), target.to(dml)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/5], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Training completed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow \n",
    "Please install tensorflow-directml-plugin. Other steps are same as standard approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure DirectML is used (if available)\n",
    "# Use the following line to list all devices and check DirectML as a device\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "\n",
    "# Optionally, if DirectML is not listed, enable it\n",
    "if any(\"DirectML\" in device.name for device in physical_devices):\n",
    "    tf.config.set_visible_devices([device for device in physical_devices if \"DirectML\" in device.name], 'GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physical_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 9s 7ms/step - loss: 0.3758 - accuracy: 0.8947\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 7s 7ms/step - loss: 0.2022 - accuracy: 0.9429\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.1507 - accuracy: 0.9571\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 6s 7ms/step - loss: 0.1217 - accuracy: 0.9658\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 7s 7ms/step - loss: 0.1020 - accuracy: 0.9710\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), _ = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28)) / 255.0\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "\n",
    "# Define a simple neural network model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(28 * 28,)),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.SGD(learning_rate=0.1),\n",
    "              loss=losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training loop\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64, verbose=1)\n",
    "\n",
    "print('Training completed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
